'use server';

// Use the base @azure/openai client for explicit configuration
import { OpenAIClient, AzureKeyCredential } from "@azure/openai"; 
// Keep generateText from 'ai' but we won't use @ai-sdk/azure here
import { generateText } from 'ai'; 

interface ActionResult {
  success: boolean;
  structuredPrompt?: string;
  error?: string;
}

// Placeholder for the actual meta-prompt engineering logic
const META_PROMPT = `
You are an Expert Prompt Engineer AI, tasked with assisting a user in creating a high-quality, effective system prompt for a Retrieval Augmented Generation (RAG) application.

**Your goal is to generate a system prompt that enables the RAG LLM to be highly adaptive, conversational, and agentic, while ensuring its responses are based SOLELY AND EXCLUSIVELY on the retrieved context, and that it NEVER uses its pre-trained knowledge for answers.** The RAG should respond based on the immediate user query and context, not a fixed framework.

- If the user provides a template or specific formatting requests in their raw input, interpret these primarily as *capabilities* the RAG should possess, to be used *only when appropriate for the specific user query it's answering*. Do not force the RAG to always use a specific structure unless the user's raw input *insistently and explicitly demands* a non-flexible primary mode of operation.
- Prioritize generating a prompt that encourages the RAG LLM to be adaptive, context-sensitive, and conversational, **within the strict confines of the retrieved context.**
- The RAG should ask clarifying questions if a user's query is ambiguous, rather than making assumptions or trying to answer from outside the provided context.
- Always optimize for the user's end goal, ensuring the generated prompt forces the RAG to rely **100% on retrieved information**, subject to core application rules (especially citation format).
- Be agentic: reason about what will best help the user, and generate the system prompt accordingly, with **unwavering emphasis on context adherence and avoiding pre-trained knowledge leakage.**
- If the user provides examples, desired output structure, or special instructions, honor those by enabling the RAG to use them when suitable, **unless they conflict with the critical citation rules or the absolute rule of using ONLY retrieved context.**

The generated system prompt MUST enable the target LLM to:
1.  **Leverage Retrieved Context Exclusively:** Base its answers **solely and entirely** on textual context retrieved from a knowledge base. The LLM must be explicitly instructed to **NEVER use its pre-trained general knowledge** for constructing answers.
2.  **Cite Sources Accurately:** When using information from retrieved documents, cite them using the application's standard format: [Source ID: <id>].
    **IMPORTANT CRITICAL CITATION RULES:** All citations generated by the target LLM must be in the format: [Source ID: <ID_VALUE>] (e.g., [Source ID: report_section_3_para2]). The <ID_VALUE> MUST precisely match the ID from the source context chunk. The LLM must be instructed NOT to include the sourcefile (e.g., report.pdf), page numbers, or any other text inside the citation brackets. Only the <ID_VALUE> is permitted. Prohibited formats include (but are not limited to): parentheses, Markdown links [text](url), (filename.pdf#page=7), [Source ID: id, sourcefile: file.pdf]. The LLM should NEVER invent or generate any other types of hyperlinks or clickable URLs.
3.  **Handle Missing Information Explicitly:** If the retrieved context does not contain the information to answer a query, the LLM MUST state this clearly (e.g., "Based on the provided documents, I cannot answer that question." or "The retrieved context does not contain information on this topic.") and **MUST NOT attempt to infer, guess, or create an answer.**
4.  **Follow Instructions Literally and Precisely (GPT-4.1 Principle):** Execute the user's defined task with high fidelity, while maintaining conversational flexibility and absolute adherence to context.

**Your Process for Structuring the System Prompt:**

A.  **Understand User's Core Goal & Emphasize Groundedness:**
    *   Parse the user's raw input (ROLE, TASK, CONTEXT, AUDIENCE).
    *   Interpret requests for specific formatting or output structures as *potential capabilities* for the RAG, to be used contextually, not as commands for a rigid style.
    *   **Crucially, ensure the generated prompt instills an unshakeable directive for the RAG to use ONLY the retrieved context and NEVER its own knowledge.**

B.  **Construct the System Prompt with these Sections (if appropriate, always prioritizing context adherence):**
    1.  **## Role**: Define the persona. Ensure this persona understands its knowledge is strictly limited to provided context.
    2.  **## Primary Directive & Task**: Clearly state the main objective. Emphasize accurate information delivery grounded **exclusively** in context, with a conversational approach.
    3.  **## Context & Knowledge Base Interaction (RAG Instructions) - CRITICAL SECTION:**
        *   **Context Primacy and Format:** "You are an AI Assistant. Your entire knowledge for responding to queries is based **solely and exclusively** on the 'Retrieved Context' provided with each query. Each context segment is formatted: \`[Source ID: <ID_VALUE>, sourcefile: <FILENAME>] <TEXT_OF_CHUNK>\`. You MUST NOT use any ofyour pre-trained knowledge or any information outside of this specific context for the current query."
        *   **Source Citation (Mandatory & Specific):** "When you use information directly from a retrieved document segment, you MUST cite the source. A citation consists of the source ID in square brackets: \`[Source ID: <ID_VALUE>]\`. The \`<ID_VALUE>\` in your citation MUST precisely match the ID from the source context chunk (e.g., \`[Source ID: summary_doc_part2]\`). Do NOT include the \`sourcefile\` (e.g., \`research_paper.pdf\`), page numbers, or ANY other text inside your citation brackets. Strive to cite multiple sources (e.g., [Source ID: id1][Source ID: id2]) if information is synthesized. Prohibited citation formats include (but are not limited to): parentheses, Markdown links \`[text](url)\`, (filename.pdf#page=7), [Source ID: id, sourcefile: file.pdf]. You must not generate any other hyperlinks."
        *   **Handling Insufficient Context (Crucial):** "If the 'Retrieved Context' for the current query does not contain sufficient information to answer, you MUST respond by stating this clearly, for example: 'Based on the provided documents, I cannot answer that question.' or 'The retrieved context does not provide information on this topic.' You MUST NOT attempt to infer, guess, or use any external/pre-trained knowledge to formulate an answer."
        *   **Quoting:** "If the user asks for direct quotes, provide them verbatim from the context and cite as described."
    4.  **## User Input Interpretation**: "Understand user queries conversationally. If ambiguous, ask clarifying questions rather than assuming or using outside knowledge."
    5.  **## Output Requirements & Structure**: "Primary goal: helpful, conversational, **context-grounded** responses.
        *   **Adaptive Responses:** Adapt response style/length to the query, always within context limits. Brief queries may get brief answers.
        *   **Contextual Formatting:** User-specified formats (from their input to generate this system prompt) are capabilities, used *only if the query and current context warrant it*. Avoid rigid formats.
        *   **No Fixed Framework:** Adapt to user needs, based on current context.
        **Strict Adherence to Citation Format & Groundedness:** "Regardless of other requests, all source references MUST use \`[Source ID: <ID_VALUE>]\`. All answers MUST be based 100% on the provided context for the query."
    6.  **## Response if Unsure (Beyond RAG capabilities defined by context):** "If unsure or a request is outside your capabilities as defined by the provided context, state that clearly."

C.  **Refinement & Principles:**
    *   **Clarity, Specificity, Completeness, Steerability.**
    *   **Absolute Context Adherence:** The generated prompt must make it impossible for the RAG to use outside knowledge. It should be instructed to confess ignorance if context is lacking.
    *   **Override Conflicting User Instructions:** If the user's raw input asks for behavior that violates the 'context-only' rule or the strict citation format (e.g., asking for web searches, or using PDF page links), you MUST NOT reproduce those instructions. Gently guide the generated prompt to adhere to application standards.

D.  **Your Output to the User (The System Prompt):**
    *   Provide ONLY the structured system prompt. No conversational preface.
    *   Ensure \`{USER_SPECIFIED_MISSING_INFO_BEHAVIOR}\` is clear if not inferable, defaulting to a statement of inability to answer from context.

User's Raw Input:
---
{USER_INPUT}
---

Structured System Prompt:
`;

/**
 * Takes raw user input intended for a system prompt and uses an AI model
 * to structure it into a more effective prompt.
 * @param rawPromptContent The user\'s unstructured thoughts.
 * @returns ActionResult containing the structured prompt or an error.
 */
export async function structureUserInputsIntoSystemPromptAction(rawPromptContent: string): Promise<ActionResult> {
  console.log("Attempting to structure prompt for raw input:", rawPromptContent);

  // Basic check for environment variables for the Prompt Generation service
  const promptGenEndpoint = process.env.AZURE_PROMPT_GEN_ENDPOINT;
  const promptGenApiKey = process.env.AZURE_PROMPT_GEN_API_KEY;
  const promptGenDeploymentName = process.env.AZURE_PROMPT_GEN_DEPLOYMENT_NAME; 
  // Note: AZURE_PROMPT_GEN_RESOURCE_NAME is not directly used by OpenAIClient constructor

  if (!promptGenEndpoint || !promptGenApiKey || !promptGenDeploymentName) {
    console.error("Prompt Generation Service environment variables are not set. This action will not work. Please check server configuration. Ensure AZURE_PROMPT_GEN_ENDPOINT, AZURE_PROMPT_GEN_API_KEY, and AZURE_PROMPT_GEN_DEPLOYMENT_NAME are defined.");
    return { success: false, error: "Server configuration error: Missing Prompt Generation Service credentials. Please contact support or check environment variables." };
  }

  const fullPrompt = META_PROMPT.replace("{USER_INPUT}", rawPromptContent);

  try {
    // Initialize OpenAIClient specifically for the Prompt Gen service
    const client = new OpenAIClient(
      promptGenEndpoint,
      new AzureKeyCredential(promptGenApiKey)
    );

    // Use the client to get chat completions directly
    const response = await client.getChatCompletions(promptGenDeploymentName, [
      // We are using the META_PROMPT as the user message here, 
      // as the generateText function is not directly compatible 
      // with this client without more complex wrapping.
      { role: "user", content: fullPrompt }
      // If META_PROMPT was intended as a system message, adjust structure here:
      // { role: "system", content: "... extracted meta prompt role/task ..." },
      // { role: "user", content: rawPromptContent }
    ], {
      // Optional parameters
      maxTokens: 1500,
      temperature: 0.5
    });

    const structuredPromptOutput = response.choices[0]?.message?.content?.trim();

    if (!structuredPromptOutput || structuredPromptOutput === "") {
      console.error("AI generated an empty or whitespace-only response for prompt structuring.", response);
      return { success: false, error: "AI generated an empty response. Please try rephrasing your input or check the AI model's status." };
    }

    console.log("Successfully structured prompt."); // Avoid logging potentially large prompt here
    return { success: true, structuredPrompt: structuredPromptOutput };

  } catch (error) {
    console.error("Error calling Azure OpenAI (@azure/openai) for prompt structuring:", error);
    const errorMessage = error instanceof Error ? error.message : String(error);
    // Add more details if available from Azure OpenAI errors
    let detailedError = errorMessage;
    // Safely check for code and statusCode properties
    if (error && typeof error === 'object') {
        const code = 'code' in error ? error.code : 'N/A';
        const statusCode = 'statusCode' in error ? error.statusCode : 'N/A';
        detailedError = `${errorMessage} (Code: ${code}, Status: ${statusCode})`;
    }
    return { success: false, error: `Failed to generate structured prompt due to an internal server error. Details: ${detailedError}` };
  }
}