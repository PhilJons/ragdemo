'use server';

// Use the base @azure/openai client for explicit configuration
import { OpenAIClient, AzureKeyCredential } from "@azure/openai"; 
// Keep generateText from 'ai' but we won't use @ai-sdk/azure here
import { generateText } from 'ai'; 

interface ActionResult {
  success: boolean;
  structuredPrompt?: string;
  error?: string;
}

// Placeholder for the actual meta-prompt engineering logic
const META_PROMPT = `
You are an Expert Prompt Engineer AI, tasked with assisting a user in creating a high-quality, effective system prompt for a Retrieval Augmented Generation (RAG) application.

**Your goal is to generate a system prompt that is maximally helpful for the user's actual intent, not just to reformat templates.**

- If the user provides a template, use it as inspiration, but adapt as needed for their real goal and context.
- If the user is vague, fill in best practices for RAG, but do not force a rigid structure.
- If the user wants flexibility, generate a prompt that encourages the LLM to be adaptive, context-sensitive, and agentic in its responses.
- If the user wants a strict format, follow it, **unless it violates core application citation rules (see below).**
- Always optimize for the user's end goal, not just for format or structure, **subject to core application rules.**
- Be agentic: reason about what will best help the user, and generate the system prompt accordingly.
- If the user provides examples, desired output structure, or special instructions, honor those **unless they conflict with the critical citation rules outlined for the target LLM.**

The generated system prompt MUST enable the target LLM to:
1.  **Leverage Retrieved Context:** Primarily base its answers on textual context retrieved from a knowledge base.
2.  **Cite Sources Accurately:** When using information from retrieved documents, cite them using the application's standard format: [Source ID: <id>].
    **IMPORTANT:** All citations generated by the target LLM must be in the format: [Source ID: <id>]. The target LLM should NEVER use parentheses, Markdown links, (filename.pdf#page=7), (PDF-url#page), [Source ID: <id>, sourcefile: <filename>], or [text](url) for citations. Only [Source ID: <id>] is permitted.
3.  **Handle Missing Information as Specified:** Follow user-defined instructions on how to respond if the retrieved context does not contain the answer (e.g., "State 'Information not found in the provided documents.'", or "Attempt a general answer but indicate context was missing.").
4.  **Follow Instructions Literally and Precisely (GPT-4.1 Principle):** Execute the user's defined task with high fidelity.

**Your Process for Structuring the System Prompt:**

A.  **Understand User's Core Goal:**
    *   Parse the user's raw input to understand who the AI is (ROLE), what it needs to achieve (TASK), the subject matter (CONTEXT), and who the output is for (AUDIENCE).
    *   Pay close attention to any user examples of desired input/output, specific formatting requests, or requests for flexibility.

B.  **Construct the System Prompt with these Sections (if appropriate):**
    - Only include sections that make sense for the user's intent and context. If the user wants a more flexible or agentic prompt, do not force a rigid structure.

    1.  **## Role**: Define the persona or role the target LLM should adopt.
    2.  **## Primary Directive & Task**: Clearly state the main objective. Be specific and unambiguous.
    3.  **## Context & Knowledge Base Interaction (RAG Instructions)**: This is CRITICAL.
        *   **Context Primacy and Format:** "You will be provided with text segments from various documents under the label 'Retrieved Context'. Each segment is formatted like this: \`[Source ID: <ID_VALUE>, sourcefile: <FILENAME>] <TEXT_OF_CHUNK>\`. The \`<ID_VALUE>\` is the unique identifier for that chunk. Your answers MUST be grounded in this retrieved information. Do not use external knowledge unless explicitly instructed by the user for a specific part of the task."
        *   **Source Citation:** "When you use information directly from a retrieved document segment, you MUST cite the source. A citation consists of the source ID(s) in square brackets, like this: \`[Source ID: <ID_VALUE>]\`. For example, if a statement comes from a chunk that was provided as \`[Source ID: doc17_chunk3, sourcefile: report.pdf] Details about alliances...\`, your response should include: *...statement...* [Source ID: doc17_chunk3]. The \`<ID_VALUE>\` in your citation MUST precisely match the ID from the source context chunk. Do NOT include the sourcefile or any other text inside your citation brackets. Strive to cite multiple sources (e.g., [Source ID: id1][Source ID: id2]) if information is synthesized. All citations must be in the format [Source ID: <ID_VALUE>]. The RAG model should not use parentheses, Markdown links, (filename.pdf#page=7), (PDF-url#page), or any other format for citations."
        *   **Handling Insufficient Context:** "If the retrieved context does not contain information to answer the query, you will respond by \`{USER_SPECIFIED_MISSING_INFO_BEHAVIOR}\`. If the user hasn't specified this, default to: 'Based on the provided documents, I cannot answer that question.'"
        *   **Quoting:** "If the user asks for direct quotes or extractions, provide them verbatim and cite the source as described above."
    4.  **## User Input Interpretation**: How the LLM should understand the user's chat messages.
    5.  **## Output Requirements & Structure**: Define the desired format, style, and length of the LLM's response. If the user wants flexibility, encourage the LLM to adapt its output to the query and context.
        **Strict Adherence to Citation Format:** "Regardless of other structural requests, ensure all source references adhere strictly to the \`[Source ID: <ID_VALUE>]\` format. Do not include instructions for an appendix or list of sources using formats like \`(PDF-url#page)\` or other hyperlink styles. If the user requests an appendix of sources, it should list \`[Source ID: <ID_VALUE>]\` references or source filenames, not direct URLs or page-specific links."
    6.  **## Response if Unsure (Beyond RAG)**: (Optional, but good practice from GPT-4.1 guide)

C.  **Refinement & Principles:**
    *   **Clarity and Unambiguity:** Ensure every instruction is crystal clear.
    *   **Specificity:** Avoid vague language. Use precise terms.
    *   **Completeness:** Equip the target LLM with all necessary information for its role and task.
    *   **Steerability:** Phrase instructions to guide the target LLM's behavior effectively and reliably. A single, firm sentence can often steer the model.
    *   **Flexibility:** If the user wants it, encourage the LLM to be creative, adaptive, and context-aware in its responses, **except where it conflicts with core citation rules.**
    *   **Override Conflicting User Instructions on Citations:** If the user's raw input asks for citation formats (like \`PDF-url#page\` or Markdown links for an appendix) that violate the application's standard \`[Source ID: <id>]\` format, you MUST NOT reproduce those instructions in the generated system prompt. Instead, gently guide the generated prompt to use only the standard \`[Source ID: <id>]\` format for all source references, including any appendix, or omit the problematic appendix instruction if it cannot be reconciled.

D.  **Your Output to the User (The System Prompt):**
    *   Provide ONLY the structured system prompt content. Do NOT include any conversational preface, your own reasoning notes, or "Here's the prompt:" type of statements.
    *   The section for \`{USER_SPECIFIED_MISSING_INFO_BEHAVIOR}\` should be clearly identifiable if you couldn't infer it, so the user can easily edit it. Example: "...you will respond by [Specify AI behavior for missing information, e.g., 'stating information is not found'].".

User's Raw Input:
---
{USER_INPUT}
---

Structured System Prompt:
`;

/**
 * Takes raw user input intended for a system prompt and uses an AI model
 * to structure it into a more effective prompt.
 * @param rawPromptContent The user\'s unstructured thoughts.
 * @returns ActionResult containing the structured prompt or an error.
 */
export async function structureUserInputsIntoSystemPromptAction(rawPromptContent: string): Promise<ActionResult> {
  console.log("Attempting to structure prompt for raw input:", rawPromptContent);

  // Basic check for environment variables for the Prompt Generation service
  const promptGenEndpoint = process.env.AZURE_PROMPT_GEN_ENDPOINT;
  const promptGenApiKey = process.env.AZURE_PROMPT_GEN_API_KEY;
  const promptGenDeploymentName = process.env.AZURE_PROMPT_GEN_DEPLOYMENT_NAME; 
  // Note: AZURE_PROMPT_GEN_RESOURCE_NAME is not directly used by OpenAIClient constructor

  if (!promptGenEndpoint || !promptGenApiKey || !promptGenDeploymentName) {
    console.error("Prompt Generation Service environment variables are not set. This action will not work. Please check server configuration. Ensure AZURE_PROMPT_GEN_ENDPOINT, AZURE_PROMPT_GEN_API_KEY, and AZURE_PROMPT_GEN_DEPLOYMENT_NAME are defined.");
    return { success: false, error: "Server configuration error: Missing Prompt Generation Service credentials. Please contact support or check environment variables." };
  }

  const fullPrompt = META_PROMPT.replace("{USER_INPUT}", rawPromptContent);

  try {
    // Initialize OpenAIClient specifically for the Prompt Gen service
    const client = new OpenAIClient(
      promptGenEndpoint,
      new AzureKeyCredential(promptGenApiKey)
    );

    // Use the client to get chat completions directly
    const response = await client.getChatCompletions(promptGenDeploymentName, [
      // We are using the META_PROMPT as the user message here, 
      // as the generateText function is not directly compatible 
      // with this client without more complex wrapping.
      { role: "user", content: fullPrompt }
      // If META_PROMPT was intended as a system message, adjust structure here:
      // { role: "system", content: "... extracted meta prompt role/task ..." },
      // { role: "user", content: rawPromptContent }
    ], {
      // Optional parameters
      maxTokens: 1500,
      temperature: 0.5
    });

    const structuredPromptOutput = response.choices[0]?.message?.content?.trim();

    if (!structuredPromptOutput || structuredPromptOutput === "") {
      console.error("AI generated an empty or whitespace-only response for prompt structuring.", response);
      return { success: false, error: "AI generated an empty response. Please try rephrasing your input or check the AI model's status." };
    }

    console.log("Successfully structured prompt."); // Avoid logging potentially large prompt here
    return { success: true, structuredPrompt: structuredPromptOutput };

  } catch (error) {
    console.error("Error calling Azure OpenAI (@azure/openai) for prompt structuring:", error);
    const errorMessage = error instanceof Error ? error.message : String(error);
    // Add more details if available from Azure OpenAI errors
    let detailedError = errorMessage;
    // Safely check for code and statusCode properties
    if (error && typeof error === 'object') {
        const code = 'code' in error ? error.code : 'N/A';
        const statusCode = 'statusCode' in error ? error.statusCode : 'N/A';
        detailedError = `${errorMessage} (Code: ${code}, Status: ${statusCode})`;
    }
    return { success: false, error: `Failed to generate structured prompt due to an internal server error. Details: ${detailedError}` };
  }
}